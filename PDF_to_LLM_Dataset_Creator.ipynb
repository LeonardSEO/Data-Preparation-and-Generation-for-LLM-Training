{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAoNs5ro45Q4eAhI3hqlfE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonardSEO/Data-Preparation-and-Generation-for-LLM-Training/blob/main/PDF_to_LLM_Dataset_Creator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEsLseuwlETT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation and Generation for LLM Training**\n",
        "\n",
        "This notebook will help you create a dataset for training a language model using text extracted from a PDF. Follow the steps below, fill in the parameters, and run each cell sequentially.\n",
        "\n",
        "## Parameters:\n",
        "1. **prompt**: Describe your dataset requirement.\n",
        "2. **temperature**: Choose a value between 0 and 1. Lower values produce more precise outputs, higher values produce more creative outputs.\n",
        "3. **number_of_examples**: Number of examples to generate (minimum 100).\n",
        "4. **pdf_url**: URL of the PDF to extract information from.\n",
        "\n",
        "### Example Parameters:\n",
        "```python\n",
        "prompt = \"A model that takes in a puzzle-like reasoning-heavy question in English, and responds with a well-reasoned, step-by-step thought out response in Spanish.\"\n",
        "temperature = 0.4\n",
        "number_of_examples = 100\n",
        "pdf_url = \"https://example.com/path/to/your/pdf.pdf\"\n"
      ],
      "metadata": {
        "id": "kHcyY2a1lGLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "prompt = \"\"  # Describe your dataset requirement here\n",
        "temperature = 0.4  # Choose a value between 0 and 1\n",
        "number_of_examples = 100  # Number of examples to generate\n",
        "pdf_url = \"\"  # URL of the PDF to extract information from"
      ],
      "metadata": {
        "id": "9fD6V1weleoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install langchain unstructured openai pybind11 chromadb Cython\n",
        "!pip install \"git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&subdirectory=PythonAPI\"\n",
        "!pip install unstructured[local-inference]\n",
        "!CC=clang CXX=clang++ ARCHFLAGS=\"-arch x86_64\" pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "!pip install layoutparser[layoutmodels,tesseract] pytesseract Pillow==9.0.0\n",
        "!apt-get install poppler-utils\n"
      ],
      "metadata": {
        "id": "4rZcDmdplX6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set OpenAI API key\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = 'your_api_key_here'\n"
      ],
      "metadata": {
        "id": "wNmDiaROlhOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "import openai\n",
        "import pandas as pd\n",
        "import json\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "import random"
      ],
      "metadata": {
        "id": "bXK8nXSvllQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load PDF and extract text\n",
        "def load_pdf_and_extract_text(pdf_url, local_pdf_path='docs/temp.pdf'):\n",
        "    !wget {pdf_url} -O {local_pdf_path}\n",
        "    text_folder = 'docs'\n",
        "    !mkdir -p {text_folder}\n",
        "    !apt-get install poppler-utils\n",
        "    loaders = [UnstructuredPDFLoader(os.path.join(text_folder, fn)) for fn in os.listdir(text_folder)]\n",
        "    index = VectorstoreIndexCreator().from_loaders(loaders)\n",
        "    return index\n",
        "\n",
        "# Example PDF URL (replace with your own PDF URL)\n",
        "index = load_pdf_and_extract_text(pdf_url)"
      ],
      "metadata": {
        "id": "n_1uPgP9lmKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to generate examples\n",
        "def generate_example(prompt, prev_examples, temperature=0.5):\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    if len(prev_examples) > 0:\n",
        "        if len(prev_examples) > 10:\n",
        "            prev_examples = random.sample(prev_examples, 10)\n",
        "        for example in prev_examples:\n",
        "            messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": example\n",
        "            })\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=1354,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message['content']\n",
        "\n",
        "# Generate examples\n",
        "prev_examples = []\n",
        "for i in range(number_of_examples):\n",
        "    print(f'Generating example {i+1}')\n",
        "    example = generate_example(prompt, prev_examples, temperature)\n",
        "    prev_examples.append(example)\n",
        "\n",
        "print(prev_examples)\n"
      ],
      "metadata": {
        "id": "7puHT6kyln0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put the examples into a dataframe and turn them into a final pair of datasets\n",
        "def prepare_data_for_jsonl(prev_examples):\n",
        "    prompts = []\n",
        "    responses = []\n",
        "    for example in prev_examples:\n",
        "        try:\n",
        "            split_example = example.split('-----------')\n",
        "            prompts.append(split_example[1].strip())\n",
        "            responses.append(split_example[3].strip())\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    data = []\n",
        "    for prompt, response in zip(prompts, responses):\n",
        "        data.append({\n",
        "            \"instruction\": prompt,\n",
        "            \"input\": \"\",\n",
        "            \"output\": response\n",
        "        })\n",
        "\n",
        "    unique_data = [dict(t) for t in {tuple(d.items()) for d in data}]\n",
        "\n",
        "    print(f'There are {len(unique_data)} successfully-generated examples. Here are the first few:')\n",
        "    for item in unique_data[:5]:\n",
        "        print(json.dumps(item, indent=4))\n",
        "\n",
        "    train_size = int(len(unique_data) * 0.9)\n",
        "    train_data = unique_data[:train_size]\n",
        "    test_data = unique_data[train_size:]\n",
        "\n",
        "    with open('train.jsonl', 'w') as train_file:\n",
        "        for entry in train_data:\n",
        "            train_file.write(json.dumps(entry) + '\\n')\n",
        "\n",
        "    with open('test.jsonl', 'w') as test_file:\n",
        "        for entry in test_data:\n",
        "            test_file.write(json.dumps(entry) + '\\n')\n",
        "\n",
        "prepare_data_for_jsonl(prev_examples)\n"
      ],
      "metadata": {
        "id": "PuVVHiROlpSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide download links for the generated files\n",
        "from google.colab import files\n",
        "files.download('train.jsonl')\n",
        "files.download('test.jsonl')\n"
      ],
      "metadata": {
        "id": "SPsNvTkdlrk9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}